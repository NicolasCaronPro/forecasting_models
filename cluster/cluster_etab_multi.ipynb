{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etabs DTW clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.metrics import cdist_dtw\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     date_entree  Total\n",
      "731   2018-01-01    217\n",
      "732   2018-01-02    280\n",
      "733   2018-01-03    251\n",
      "734   2018-01-04    239\n",
      "735   2018-01-05    217\n",
      "...          ...    ...\n",
      "2917  2023-12-27    330\n",
      "2918  2023-12-28    280\n",
      "2919  2023-12-29    282\n",
      "2920  2023-12-30    286\n",
      "2921  2023-12-31    192\n",
      "\n",
      "[2191 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "etabs = ['CH Beaune',\n",
    "        'CH Chatillon Montbard',\n",
    "        'CH Chaumont',\n",
    "        'CH Langres',\n",
    "        'CH privé Dijon',\n",
    "        'CH Semur',\n",
    "        'CHU Besançon',\n",
    "        'CHU Dijon',\n",
    "        'HNFC']\n",
    "\n",
    "fake_data = False\n",
    "fake_etabs = False\n",
    "\n",
    "if fake_etabs:\n",
    "    for i in range(len(etabs)//4):\n",
    "        etabs.append('CH fake '+str(i))\n",
    "\n",
    "\n",
    "data = []\n",
    "for etab in etabs:\n",
    "    \n",
    "    # fecthing data\n",
    "    if not fake_etabs:\n",
    "        data.append(pd.read_excel('../data/features/hopitalfeatures/Export complet '+etab+'.xlsx', sheet_name='Volumes', usecols='A:B'))\n",
    "    else:\n",
    "        np.random.seed(etabs.index(etab))\n",
    "        df = pd.DataFrame()\n",
    "        df['date_entree'] = pd.date_range(start='2018-01-01', end='2022-12-31', freq='D')\n",
    "        data.append(df)\n",
    "        data[-1]['Total'] = np.random.randint(0, 100, size=len(data[-1]))\n",
    "\n",
    "    # fake data\n",
    "    if fake_data:\n",
    "        np.random.seed(etabs.index(etab))\n",
    "        data[-1].loc[data[-1][\"Total\"] == 0, \"Total\"] = 15\n",
    "        data[-1][\"hospit\"] = data[-1][\"Total\"].values\n",
    "        data[-1][\"hospit\"] -= np.random.randint(0, data[-1][\"Total\"].min(), size=len(data[-1]))\n",
    "        #data[-1][\"hospit\"] = np.random.randint(0, data[-1][\"Total\"], size=len(data[-1]))\n",
    "        data[-1][\"air\"] = np.random.normal(0, 10, size=len(data[-1]))\n",
    "        data[-1][\"temperature\"] = np.random.normal(0, 30, size=len(data[-1]))\n",
    "        data[-1][\"accident\"] = np.random.normal(120, 240, size=len(data[-1]))\n",
    "    \n",
    "    data[-1] = data[-1].loc[data[-1]['date_entree'] >= '2018-01-01']\n",
    "\n",
    "print(data[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each hospital's DataFrame to a 2D numpy array (n_series, n_timepoints) for each hospital\n",
    "hospital_data = []\n",
    "for i in range(len(etabs)):\n",
    "    hospital_data.append(data[i].drop(columns='date_entree'))\n",
    "\n",
    "time_series_data = np.array(hospital_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume time_series_data is a 3D array (n_samples, n_timepoints, n_features)\n",
    "# Reshape to 2D for scaling\n",
    "n_samples, n_timepoints, n_features = time_series_data.shape\n",
    "reshaped_data = time_series_data.reshape(n_samples * n_timepoints, n_features)\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(reshaped_data)\n",
    "\n",
    "# Reshape back to original 3D shape\n",
    "scaled_time_series_data = scaled_data.reshape(n_samples, n_timepoints, n_features)\n",
    "time_series_data = scaled_time_series_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal number of clusters (silhouette score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 2, Silhouette Score: 0.868\n",
      "Number of clusters: 3, Silhouette Score: 0.690\n",
      "Number of clusters: 4, Silhouette Score: 0.364\n",
      "Number of clusters: 5, Silhouette Score: 0.256\n",
      "Number of clusters: 6, Silhouette Score: 0.211\n",
      "Number of clusters: 7, Silhouette Score: 0.115\n",
      "Number of clusters: 8, Silhouette Score: 0.081\n",
      "\n",
      "Optimal number of clusters: 2 -> [0 0 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Range of clusters to try\n",
    "range_n_clusters = range(2, len(etabs))\n",
    "\n",
    "# Placeholder for silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# Placeholder for labels\n",
    "labels = []\n",
    "\n",
    "# Loop over cluster sizes to find the optimal number\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Apply DTW KMeans clustering\n",
    "    km_dtw = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", random_state=0)\n",
    "    labels.append(km_dtw.fit_predict(time_series_data))\n",
    "    \n",
    "    # Calculate the pairwise DTW distance matrix\n",
    "    distance_matrix = cdist_dtw(time_series_data)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(distance_matrix, labels[-1], metric=\"precomputed\")\n",
    "    silhouette_scores.append((n_clusters, silhouette_avg))\n",
    "    \n",
    "    print(f\"Number of clusters: {n_clusters}, Silhouette Score: {silhouette_avg:.3f}\")\n",
    "\n",
    "# Select the number of clusters with the highest silhouette score\n",
    "best_n_clusters = max(silhouette_scores, key=lambda x: x[1])[0]\n",
    "print(f\"\\nOptimal number of clusters: {best_n_clusters} -> {labels[best_n_clusters-2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output clusters of etabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "['CH Beaune', 'CH Chatillon Montbard', 'CH Chaumont', 'CH Langres', 'CH privé Dijon', 'CH Semur']\n",
      "Cluster 1\n",
      "['CHU Besançon', 'CHU Dijon', 'HNFC']\n"
     ]
    }
   ],
   "source": [
    "# Output the cluster labels for each hospital\n",
    "clusters = {}\n",
    "for i in range(len(etabs)):\n",
    "    try:\n",
    "        clusters[labels[i]].append(etabs[i])\n",
    "    except KeyError:\n",
    "        clusters[labels[i]] = [etabs[i]]\n",
    "for cluster in clusters:\n",
    "    print(\"Cluster\", cluster)\n",
    "    print(clusters[cluster])\n",
    "#print(clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Placeholder for feature extraction (mean, variance for simplicity)\n",
    "# list_features = ['Total', 'air', 'temperature', 'population']\n",
    "# features = []\n",
    "# for df in data:\n",
    "#     mean = []\n",
    "#     std_dev = []\n",
    "#     for feature in list_features:\n",
    "#         mean.append(df[feature].mean())   # Mean for each time series\n",
    "#         std_dev.append(df[feature].std())\n",
    "#     features.append([mean.mean(), std_dev.mean()])  # Aggregating into a feature vector\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# features_df = pd.DataFrame(features, columns=['Mean', 'Std_Dev'])\n",
    "\n",
    "# # Normalize features\n",
    "# scaler = StandardScaler()\n",
    "# features_scaled = scaler.fit_transform(features_df)\n",
    "\n",
    "# # Apply K-means clustering\n",
    "# kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "# labels = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# # Assign labels back to the hospitals\n",
    "# features_df['Cluster'] = labels\n",
    "\n",
    "# # Display results\n",
    "# print(features_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
