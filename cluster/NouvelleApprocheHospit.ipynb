{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction des arrivées aux urgences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import statistics\n",
    "import xgboost as xgb\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_target = 'hospitalisations'\n",
    "assert type_target in ['hospitalisations', 'arrivees_urgences']\n",
    "horizon = 15\n",
    "NUM_FEATS = 50\n",
    "DEPARTEMENT = 'Dijon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julienaudoux/Desktop/WORK(lab)/forecasting_models/.venv/lib/python3.10/site-packages/IPython/core/magics/pylab.py:162: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (18,8)\n",
    "\n",
    "def teste(df, params, score=100000, ma=7, verbose=False):\n",
    "    MAEs, MSEs = [], []\n",
    "    new_score = 100000\n",
    "    reg_best = None\n",
    "    final = df.loc[df.index.year==2023]\n",
    "    X_final = final.drop('target', axis=1)\n",
    "    y_final = final['target']\n",
    "\n",
    "    for max_depth in [3, 4, 5, 6, 7, 8, 9]:\n",
    "        params['max_depth'] = max_depth\n",
    "        train_val_test = df.copy()\n",
    "        for annee in range(2019, 2024):\n",
    "            train_val = train_val_test.loc[train_val_test.index.year != annee]\n",
    "            test = train_val_test.loc[train_val_test.index.year == annee]\n",
    "            X_test = test.drop('target', axis=1)\n",
    "            y_test = test['target']\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(train_val.drop('target', axis=1), train_val['target'], \n",
    "                                                                  test_size=0.2, shuffle=False)\n",
    "                    \n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "            dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "            evals = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "            bst = xgb.train(params, dtrain, num_boost_round=100000, evals=evals,\n",
    "                            early_stopping_rounds=15, verbose_eval=verbose)\n",
    "        \n",
    "            y_pred = bst.predict(dtest)\n",
    "            MAEs.append(mean_absolute_error(y_test, y_pred))\n",
    "            MSEs.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "        current_score = statistics.mean(MSEs)\n",
    "        if current_score < new_score:\n",
    "            train_val_test = df.loc[df.index.year<2023]\n",
    "            previous_score = new_score\n",
    "            new_score = current_score\n",
    "            if new_score < score:\n",
    "                print(f\"Amélioration avec {max_depth=}: {min(previous_score, score):.4f} -> {new_score:.4f}\")\n",
    "                new_score = statistics.mean(MSEs)\n",
    "                X_train, X_valid, y_train, y_valid = train_test_split(train_val_test.drop('target', axis=1), \n",
    "                                                                    train_val_test['target'], \n",
    "                                                                    test_size=0.2, shuffle=False)\n",
    "\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "                dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "                dtest = xgb.DMatrix(X_final, label=y_final)\n",
    "                evals = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "                bst = xgb.train(params, dtrain, num_boost_round=100000, evals=evals,\n",
    "                                early_stopping_rounds=15, verbose_eval=verbose)\n",
    "                reg_best = bst\n",
    "                y_pred = bst.predict(dtest)\n",
    "                #y_pred_ma = pd.Series(y_pred).rolling(window=ma, center=True).mean()\n",
    "                N = 365\n",
    "                plt.figure()\n",
    "                plt.plot(range(len(y_final))[-N:], y_final[-N:], label='actual', color='blue')\n",
    "                plt.plot(range(len(y_pred))[-N:], y_pred[-N:], label='predict', color='orange')\n",
    "                #plt.plot(range(len(y_pred))[-N:], y_pred_ma[-N:], label='trend', color='red')\n",
    "                plt.legend()\n",
    "                '''y_test_ma = y_final.rolling(window=ma, center=True).mean()\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.plot(range(len(y_final)), y_test_ma, label='actual')\n",
    "                plt.plot(range(len(y_pred)), y_pred_ma, label='predict')\n",
    "                plt.legend()\n",
    "                '''\n",
    "                display(plt.gcf())\n",
    "\n",
    "                mean_y_test = np.mean(y_final)\n",
    "                error_percentage = np.abs((y_pred - y_final) / mean_y_test) * 100\n",
    "                percent_below_thresholds = {}\n",
    "                for threshold in range(1, 12):  # de 1% à 30%\n",
    "                    percent_below = np.mean(error_percentage < threshold) * 100\n",
    "                    percent_below_thresholds[threshold] = percent_below\n",
    "                for threshold, percent_below in percent_below_thresholds.items():\n",
    "                    print(f\"{percent_below:.2f}%,{threshold}%\")\n",
    "\n",
    "                print(f\"   - MAE : {statistics.mean(MAEs):.2f} ({mean_absolute_error(y_pred, y_final):.2f} sur 2023)\")\n",
    "                print(f\"   - MSE : {statistics.mean(MSEs):.2f} ({mean_squared_error(y_pred, y_final):.2f} sur 2023)\")\n",
    "    return new_score, reg_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"43466\" doesn't match format \"%d/%m/%Y\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_target \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhospitalisations\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      3\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRPU_vers_hospit_adultes.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_entree\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate_entree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm/\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     df\u001b[38;5;241m.\u001b[39mrename({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m'\u001b[39m: type_target}, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[type_target]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Desktop/WORK(lab)/forecasting_models/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m~/Desktop/WORK(lab)/forecasting_models/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[1;32m    436\u001b[0m     arg,\n\u001b[1;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/WORK(lab)/forecasting_models/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[1;32m    457\u001b[0m     arg,\n\u001b[1;32m    458\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data \"43466\" doesn't match format \"%d/%m/%Y\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "assert type_target in ['hospitalisations', 'arrivees_urgences']\n",
    "if type_target == 'hospitalisations':\n",
    "    df = pd.read_excel('RPU_vers_hospit_adultes.xlsx')\n",
    "    df['date_entree'] = pd.to_datetime(df['date_entree'], unit='D', origin='1899-12-30')\n",
    "    df.rename({'Total': type_target}, axis=1, inplace=True)\n",
    "    df['target'] = df[type_target].copy()\n",
    "    # on ajoute les arrivées aux urgences, que l'on suppose connues pour le jour J\n",
    "    dff = pd.read_feather('CHU Dijon_volumes.feather')\n",
    "    dff.rename({'Total': 'arrivees_urgences'}, axis=1, inplace=True)\n",
    "    dff.reset_index(inplace=True)\n",
    "    df = pd.merge(df, dff, on='date_entree')\n",
    "else:\n",
    "    df = pd.read_feather('CHU Dijon_volumes.feather')\n",
    "    df.rename({'Total': type_target}, axis=1, inplace=True)\n",
    "    df['target'] = df[type_target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ajoute la target\n",
    "if type_target == 'hospitalisations':\n",
    "    horizon = 7\n",
    "else:\n",
    "    horizon = 3\n",
    "df['mean'] = df[type_target].rolling(window=horizon, min_periods=1).mean()\n",
    "df['target'] = df['mean'].shift(-2*horizon+1)\n",
    "df.drop('mean', axis=1, inplace=True)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2*horizon-1, 15):\n",
    "    df[f'target_history-{k}'] = df['target'].shift(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 15):\n",
    "    df[f'{type_target}_history-{k}'] = df[type_target].shift(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = pd.read_csv('features_all_origin.csv', sep=',')\n",
    "dg.drop('Total', axis=1, inplace=True)\n",
    "dg['date_entree'] = pd.to_datetime(dg['date_entree'], format=\"%Y-%m-%d\")\n",
    "df = pd.merge(df, dg, on='date_entree', how='left')\n",
    "df.set_index('date_entree', inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Moyenne : {df['target'].mean():.2f}, Ecart-type : {df['target'].std():.2f}\")\n",
    "print(f\"Baseline : {mean_absolute_error(df['target'], [df['target'].mean()]*len(df)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HNFC_moving'] = df['HNFC_moving']=='Après'\n",
    "df.drop('nom_etablissement', axis=1, inplace=True)\n",
    "df = df.drop(df.loc['2020-03':'2020-05'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted = df[\"target\"].shift(2*horizon-1)\n",
    "for ma in [2, 3, 7, 15, 30]:    \n",
    "    window = shifted.rolling(window=ma)\n",
    "    df[f'target_ma_{ma}'] = window.mean()\n",
    "    df.bfill(inplace=True)\n",
    "\n",
    "shifted = df[type_target].shift(1)\n",
    "for ma in [2, 3, 7, 15, 30]:    \n",
    "    window = shifted.rolling(window=ma)\n",
    "    df[f'{type_target}_ma_{ma}'] = window.mean()\n",
    "    df.bfill(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sauvegarde notre dataframe\n",
    "df0 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On va commencer par prédire la tendance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target']\n",
    "result = seasonal_decompose(y, model='additive', period=7)\n",
    "\n",
    "# Affichage des composantes\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(411)\n",
    "plt.plot(y, label='Série temporelle')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(412)\n",
    "plt.plot(result.trend, label='Tendance')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(413)\n",
    "plt.plot(result.seasonal, label='Saisonnalité')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(414)\n",
    "plt.plot(result.resid, label='Résidu')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df0.copy()\n",
    "df['target'] = result.trend.copy()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eta': 0.05, \n",
    "          'objective': 'reg:absoluteerror', \n",
    "          'eval_metric': ['rmse', 'mae'],\n",
    "          'subsample': 0.7, \n",
    "          'colsample_bytree': 1,\n",
    "          'nthread': -1}\n",
    "\n",
    "new_score, bst = teste(df, params)\n",
    "\n",
    "# Un exemple pour déterminer l'importance des variables après un pré-apprentissage XGBoost\n",
    "importance_gain = bst.get_score(importance_type='gain')\n",
    "importance_cover = bst.get_score(importance_type='cover')\n",
    "importance_weight = bst.get_score(importance_type='weight')\n",
    "\n",
    "df_gain = pd.DataFrame.from_dict(importance_gain, orient='index', columns=['gain'])\n",
    "df_cover = pd.DataFrame.from_dict(importance_cover, orient='index', columns=['cover'])\n",
    "df_weight = pd.DataFrame.from_dict(importance_weight, orient='index', columns=['weight'])\n",
    "\n",
    "df = df_gain.join(df_cover, how='outer').join(df_weight, how='outer')\n",
    "df.fillna(0, inplace=True)  # Remplacer les valeurs manquantes par 0 si nécessaire\n",
    "\n",
    "df = df_gain.join(df_cover, how='outer').join(df_weight, how='outer')\n",
    "df.fillna(0, inplace=True)  # Remplacer les valeurs manquantes par 0 si nécessaire\n",
    "\n",
    "df['gain_norm'] = df['gain'] / df['gain'].sum()\n",
    "df['cover_norm'] = df['cover'] / df['cover'].sum()\n",
    "df['weight_norm'] = df['weight'] / df['weight'].sum()\n",
    "\n",
    "w_gain = 0.5\n",
    "w_cover = 0.3\n",
    "w_weight = 0.2\n",
    "\n",
    "df['importance'] = (df['gain_norm'] * w_gain) + (df['cover_norm'] * w_cover) + (df['weight_norm'] * w_weight)\n",
    "\n",
    "df.sort_values(by='importance', ascending=False, inplace=True)\n",
    "df['rank'] = df['importance'].rank(ascending=False)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'feature'}, inplace=True)\n",
    "print(df[['feature', 'gain', 'cover', 'weight', 'importance', 'rank']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in sorted(df.columns):\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.plot(y=['target', 'target_history-14'])\n",
    "df.plot(y=['target', 'target_ma_2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevision.iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# Exemple de série temporelle synthétique (à remplacer par vos données)\n",
    "#date_rng = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "#data = np.sin(2 * np.pi * date_rng.dayofyear / 365) + np.random.normal(0, 0.1, len(date_rng))\n",
    "\n",
    "cible = []\n",
    "\n",
    "N = 365\n",
    "p, d, q = 4,1,3\n",
    "for l in range(N):\n",
    "    liste = list(df['target'][:(-(N+15)+l)].values)\n",
    "    for k in range(14):\n",
    "        # Définir le modèle ARMA (p, q à ajuster selon votre série temporelle)\n",
    "        #modele = ARIMA(liste, order=(p, d, q), enforce_stationarity=False, enforce_invertibility=False)\n",
    "        modele = SARIMAX(liste, order=(4, 1, 3), seasonal_order=(1, 1, 0, 7))\n",
    "        # Ajuster le modèle\n",
    "        modele_fit = modele.fit(method_kwargs={\"disp\": 0})\n",
    "        prevision = modele_fit.forecast(steps=1)\n",
    "        liste.append(prevision[0])\n",
    "        # Affichage des résultats\n",
    "        if k == 13:\n",
    "            print(l, prevision[0])\n",
    "            cible.append(prevision[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "# Recherche des meilleurs paramètres ARIMA\n",
    "modele_auto = auto_arima(serie, seasonal=False, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "print(modele_auto.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(cible, list(df['target'][-N:].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cible, label='prédit')\n",
    "plt.plot(list(df['target'][-N:].values), label='réel')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "cible = []\n",
    "\n",
    "\n",
    "N = 365\n",
    "for k in range(N):\n",
    "    serie = df['target'][:(-(N+15)+k)]\n",
    "\n",
    "\n",
    "    # Définir le modèle ARMA (p, q à ajuster selon votre série temporelle)\n",
    "    #p, d, q =4,1,3\n",
    "    #modele = ARIMA(serie, order=(p, d, q), enforce_stationarity=False, enforce_invertibility=False)\n",
    "\n",
    "    modele = SARIMAX(serie, order=(4, 1, 3), seasonal_order=(1, 1, 0, 7))\n",
    "    modele_fit = modele.fit(disp=False)\n",
    "\n",
    "\n",
    "    # Ajuster le modèle\n",
    "    #modele_fit = modele.fit(method_kwargs={\"disp\": 0})\n",
    "\n",
    "    # Prédiction des 15 prochains jours\n",
    "    pas_prevision = 15\n",
    "    prevision = modele_fit.forecast(steps=pas_prevision)\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(k, prevision.iloc[-1])\n",
    "    cible.append(prevision.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(serie, label='Série temporelle')\n",
    "plt.plot(previsions_series, label='Prévisions', linestyle='--')\n",
    "plt.title('Prédictions avec ARMA')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df0.copy()\n",
    "df['target'] = result.trend.copy()\n",
    "df.dropna(inplace=True)\n",
    "#target_history-13\n",
    "#target_history-14\n",
    "teste(df[['target_ma_2', 'target_history-14', 'target']], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eta': 0.05, \n",
    "          'objective': 'reg:absoluteerror', \n",
    "          'eval_metric': ['rmse', 'mae'],\n",
    "          'subsample': 0.7, \n",
    "          'colsample_bytree': 1,\n",
    "          'nthread': -1}\n",
    "\n",
    "score = 100000\n",
    "cols = ['dayofYear', 'month']\n",
    "col = \"holidays\"\n",
    "new_score, _ = teste(df[cols+[col, 'target']], params, score=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eta': 0.05, \n",
    "\t\t  'objective': 'reg:squarederror',\n",
    "          'eval_metric': ['rmse', 'mae'], \n",
    "          # seule la rmse sera utilisée pour valider\n",
    "          'subsample': 0.7, 'colsample_bytree': 0.7,\n",
    "          'nthread': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['dayofYear', 'month', 'holidays', 'arrivees_urgences']\n",
    "score, reg = teste(df[cols+['target']], params)\n",
    "\n",
    "dg = df0.loc[df0.index.year == 2023][:-3]\n",
    "dg['pred'] = reg.predict(xgb.DMatrix(df.loc[df.index.year == 2023][cols]))\n",
    "dg.plot(y=['target', 'pred'])\n",
    "print(f\"MAE: {mean_absolute_error(dg['target'], dg['pred']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['dayofYear', 'month', 'holidays', 'arrivees_urgences', 'arrivees_urgences_history-1']\n",
    "score, reg = teste(df[cols+['target']], params)\n",
    "\n",
    "dg = df0.loc[df0.index.year == 2023][:-3]\n",
    "dg['pred'] = reg.predict(xgb.DMatrix(df.loc[df.index.year == 2023][cols]))\n",
    "dg.plot(y=['target', 'pred'])\n",
    "print(f\"MAE: {mean_absolute_error(dg['target'], dg['pred']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg['seasonal'] = result.seasonal\n",
    "dg['pred'] = dg.apply(lambda x: x['pred'] + x['seasonal'], axis=1)\n",
    "dg.plot(y=['target', 'pred'])\n",
    "print(f\"MAE: {mean_absolute_error(dg['target'], dg['pred']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg0 = df0.copy()\n",
    "dg0['resid'] = result.resid\n",
    "dg0.drop('target', axis=1, inplace=True)\n",
    "dg0.rename({'resid': 'target'}, axis=1, inplace=True)\n",
    "dg0.dropna(inplace=True)\n",
    "dg0['target'] = dg0['target'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres adaptés pour la classification binaire\n",
    "params = {\n",
    "    'eta': 0.05, \n",
    "    'objective': 'binary:logistic',  # Classification binaire\n",
    "    'eval_metric': ['logloss', 'error'],  # Log loss et taux d'erreur\n",
    "    'subsample': 0.7, \n",
    "    'colsample_bytree': 0.7,\n",
    "    'nthread': -1\n",
    "}\n",
    "\n",
    "X_final = dg0.loc[dg0.index.year == 2023].drop('target', axis=1)\n",
    "y_final = dg0.loc[dg0.index.year == 2023]['target']\n",
    "\n",
    "train_val_test = dg0.loc[dg0.index.year < 2023]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_val_test.drop('target', axis=1), \n",
    "                                                    train_val_test['target'], \n",
    "                                                    test_size=0.2, shuffle=False)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "dtest = xgb.DMatrix(X_final, label=y_final)\n",
    "evals = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "\n",
    "# Entraînement avec early stopping\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,  # DMatrix contenant les données d'entraînement\n",
    "    num_boost_round=100000,\n",
    "    evals=evals,  # Liste des ensembles d'évaluation\n",
    "    early_stopping_rounds=15,\n",
    "    verbose_eval=True\n",
    ")\n",
    "\n",
    "# Prédiction des probabilités sur un ensemble de test\n",
    "proba_preds = bst.predict(dtest)\n",
    "\n",
    "# Conversion des probabilités en classes binaires (0 ou 1)\n",
    "class_preds = (proba_preds >= 0.5).astype(int)\n",
    "\n",
    "scale = result.resid.dropna().abs().mean()\n",
    "\n",
    "class_preds = [(2*p-1)*scale for p in class_preds]\n",
    "\n",
    "# Affichage des prédictions de classes\n",
    "print(class_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(class_preds)), class_preds)\n",
    "plt.plot(range(len(class_preds)), result.resid[-len(class_preds):])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg['residue'] = class_preds\n",
    "dg['pred'] = dg.apply(lambda x: x['pred'] + x['residue'], axis=1)\n",
    "dg.plot(y=['target', 'pred'])\n",
    "print(f\"MAE: {mean_absolute_error(dg['target'], dg['pred']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un exemple pour déterminer l'importance des variables après un pré-apprentissage XGBoost\n",
    "importance_gain = bst.get_score(importance_type='gain')\n",
    "importance_cover = bst.get_score(importance_type='cover')\n",
    "importance_weight = bst.get_score(importance_type='weight')\n",
    "\n",
    "df_gain = pd.DataFrame.from_dict(importance_gain, orient='index', columns=['gain'])\n",
    "df_cover = pd.DataFrame.from_dict(importance_cover, orient='index', columns=['cover'])\n",
    "df_weight = pd.DataFrame.from_dict(importance_weight, orient='index', columns=['weight'])\n",
    "\n",
    "df = df_gain.join(df_cover, how='outer').join(df_weight, how='outer')\n",
    "df.fillna(0, inplace=True)  # Remplacer les valeurs manquantes par 0 si nécessaire\n",
    "\n",
    "df = df_gain.join(df_cover, how='outer').join(df_weight, how='outer')\n",
    "df.fillna(0, inplace=True)  # Remplacer les valeurs manquantes par 0 si nécessaire\n",
    "\n",
    "df['gain_norm'] = df['gain'] / df['gain'].sum()\n",
    "df['cover_norm'] = df['cover'] / df['cover'].sum()\n",
    "df['weight_norm'] = df['weight'] / df['weight'].sum()\n",
    "\n",
    "w_gain = 0.5\n",
    "w_cover = 0.3\n",
    "w_weight = 0.2\n",
    "\n",
    "df['importance'] = (df['gain_norm'] * w_gain) + (df['cover_norm'] * w_cover) + (df['weight_norm'] * w_weight)\n",
    "\n",
    "df.sort_values(by='importance', ascending=False, inplace=True)\n",
    "df['rank'] = df['importance'].rank(ascending=False)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'feature'}, inplace=True)\n",
    "print(df[['feature', 'gain', 'cover', 'weight', 'importance', 'rank']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
