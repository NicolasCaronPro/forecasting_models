{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "\n",
    "Placer les fichier ```new_google_tool.py``` et ```new_google_wrapper.py``` dans le même dossier que ce script\n",
    "\n",
    "Pour le moment les classes n'ont été que très légèrements modifiées.\n",
    "\n",
    "Dans le futur on pourrait y mettre le code de la cellule suivante SerpAPI\n",
    "\n",
    "Changer la clé d'API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: vomissements\n",
      "Date From: Jan 1,, 2018\n",
      "Min Value: 0\n",
      "Max Value: 100\n",
      "Average Value: 41.837037037037035\n",
      "Percent Change: -37.93103448275862%\n",
      "Trend values: 87, 100, 91, 63, 82, 51, 62, 68, 46, 0, 58, 57, 95, 86, 58, 63, 62, 75, 40, 48, 57, 67, 55, 71, 86, 56, 80, 95, 54, 0, 75, 78, 0, 68, 63, 0, 50, 43, 54, 0, 46, 0, 78, 85, 48, 75, 72, 69, 94, 49, 66, 54, 45, 0, 53, 49, 53, 96, 49, 0, 55, 82, 98, 46, 58, 39, 67, 77, 94, 55, 51, 0, 41, 61, 0, 49, 0, 88, 42, 56, 43, 53, 46, 61, 45, 51, 37, 76, 44, 0, 75, 82, 62, 39, 0, 69, 62, 69, 0, 100, 0, 62, 52, 0, 73, 64, 57, 0, 56, 0, 0, 47, 50, 0, 48, 0, 55, 44, 44, 48, 0, 41, 66, 76, 49, 48, 42, 0, 63, 90, 0, 52, 72, 49, 38, 43, 41, 57, 0, 0, 0, 0, 63, 0, 0, 53, 0, 43, 58, 0, 58, 0, 77, 0, 0, 51, 0, 0, 51, 65, 66, 52, 57, 41, 0, 44, 62, 0, 0, 49, 39, 0, 56, 0, 0, 57, 0, 0, 54, 40, 0, 0, 41, 52, 44, 0, 0, 83, 0, 0, 0, 39, 65, 56, 0, 0, 0, 0, 43, 46, 68, 0, 71, 51, 50, 0, 49, 68, 0, 55, 48, 42, 0, 41, 65, 56, 62, 46, 0, 42, 51, 52, 53, 81, 0, 44, 50, 0, 0, 0, 65, 50, 0, 0, 47, 55, 57, 56, 64, 51, 0, 57, 47, 0, 56, 40, 40, 44, 66, 0, 52, 0, 46, 59, 41, 0, 0, 47, 0, 60, 0, 38, 50, 41, 0, 46, 39, 74, 79, 54\n",
      "Rising Related Queries: vomissements fecaloïdes, norovirus, que manger en cas de diarrhée et vomissements, quoi manger apres vomissements, bronchite et vomissements, que manger apres vomissements, stopper les vomissements naturellement, calmer les vomissements gastro, intoxication alimentaire sans vomissements, toux et vomissements, toux vomissements\n",
      "Top Related Queries: les vomissements, vomissement, vomissements enfant, gastro, gastro vomissements, diarrhée, vomissements diarrhée, vomissements grossesse, vomissements et diarrhée, contre les vomissements, toux vomissements, vomissements chien, vomissements chat, vomissements enfants, vertiges vomissements, covid vomissements, nausées et vomissements, nausees, vomissements bébé, covid, intoxication alimentaire, vomissement enfant, anti vomissements, stopper vomissements, vomissements sans fièvre\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from new_google_tool import MyGoogleTrendsQueryRun as GoogleTrendsQueryRun\n",
    "from new_google_wrapper import MyGoogleTrendsAPIWrapper as GoogleTrendsAPIWrapper\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"cf7a3d5a2dd56565ef56451a80d9ea4a5540690bc11c12fc84417be5b4e17f27\"\n",
    "tool = GoogleTrendsQueryRun(api_wrapper=GoogleTrendsAPIWrapper())\n",
    "\n",
    "print(tool.run({\"query\":\"vomissements\", \"geo\":\"FR\", \"date\":\"2018-01-01 2018-09-27\", \"tz\":\"2\", \"data_type\":\"TIMESERIES\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SerpAPI\n",
    "\n",
    "Nombre de requête limité à 100 par mois\n",
    "\n",
    "Changer la clé API et adatper les autres paramètres selon le besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "# logger.info(\"On charge les données de Google Trends depuis le fichier\")\n",
    "all_words_df_2024 = pd.read_csv('/home/maxime/Documents/WORKSPACES/ARS_Project/data/features/google_trends/trends_2019_2024.csv', parse_dates=['date_entree'])\n",
    "all_words_df_2018 = pd.read_csv('/home/maxime/Documents/WORKSPACES/ARS_Project/data/features/google_trends/trends_2018.csv', parse_dates=['date_entree'])\n",
    "all_words_df_2018.date_entree = all_words_df_2018.date_entree.dt.date\n",
    "all_words_df_2024.date_entree = all_words_df_2024.date_entree.dt.date\n",
    "all_words_df = pd.concat([all_words_df_2018.loc[all_words_df_2018.date_entree < dt.datetime(2019, 1, 1).date()], all_words_df_2024])\n",
    "all_words_df.to_csv('/home/maxime/Documents/WORKSPACES/ARS_Project/data/features/google_trends/trends.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "BATCH_SIZE = 270\n",
    "START = dt.datetime(2018, 1, 1)\n",
    "STOP_DATA = dt.datetime(2024, 1, 1)\n",
    "API_KEY = \"2db567698593dfdbc27232e2244cd1346cac3c105e2b93f11334db10e8536bde\"\n",
    "\n",
    "bow = [\n",
    "    \"diarrhée\", \"vomissements\", \"toux\", \"éruption cutanée\", \"infection urinaire\",\n",
    "    \"hopital\", \"médecin\", \"pharmacie\", \"médicament\", \"vaccin\", \"maladie\", \"fièvre\",\n",
    "    \"grippe\", \"rhume\", \"angine\", \"otite\", \"allergie\", \"asthme\", \"diabète\", \"obésité\",\n",
    "    \"tabac\", \"alcool\", \"drogue\", \"stress\", \"cancer\", \"dépression\", \"mal de tête\",\n",
    "    \"douleur thoracique\", \"palpitations\", \"essoufflement\", \"vertiges\", \"crampes abdominales\",\n",
    "    \"saignements\", \"douleur abdominale\", \"hypothermie\", \"hyperthermie\", \"appendicite\",\n",
    "    \"méningite\", \"pneumonie\", \"maladie cardiaque\", \"AVC\", \"infection respiratoire\",\n",
    "    \"gastro-entérite\", \"infection cutanée\", \"insuffisance cardiaque\", \"maladie rénale\",\n",
    "    \"épilepsie\", \"migraines\", \"maladie de Crohn\", \"colite ulcéreuse\", \"accident de voiture\",\n",
    "    \"fracture\", \"entorse\", \"brûlure\", \"empoisonnement\", \"chute\", \"noyade\", \"asphyxie\",\n",
    "    \"crise de panique\", \"schizophrénie\", \"trouble bipolaire\", \"démence\", \"tentative de suicide\",\n",
    "    \"urgences\", \"urgence médicale\", \"service d'urgence\", \"douleur\", \"S.O.S. médecin\"\n",
    "]\n",
    "\n",
    "def date_range(start_date, stop_date, batch_size):\n",
    "    current_date = start_date\n",
    "    while current_date <= stop_date:\n",
    "        yield current_date\n",
    "        current_date += dt.timedelta(days=batch_size)\n",
    "\n",
    "def get_all_dates_df(bow, i, start, end, batch_size):\n",
    "    all_dates_df = pd.DataFrame()\n",
    "    for date in date_range(start, end, batch_size):\n",
    "        end_date = min(date + dt.timedelta(days=batch_size-1), end)\n",
    "\n",
    "        group = ', '.join(bow[i:i+5])\n",
    "        print(f\"Récupération de {group} pour la période {date} - {end_date}\")\n",
    "\n",
    "        params = {\n",
    "            \"engine\": \"google_trends\",\n",
    "            \"q\": f\"{group}\",\n",
    "            \"geo\": \"FR\",\n",
    "            \"date\": f\"{date.strftime('%Y-%m-%d')} {end_date.strftime('%Y-%m-%d')}\",\n",
    "            \"tz\": \"0\",\n",
    "            \"data_type\": \"TIMESERIES\",\n",
    "            \"api_key\": API_KEY\n",
    "        }\n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "        try:\n",
    "            interest_over_time = results[\"interest_over_time\"][\"timeline_data\"]\n",
    "            df = pd.DataFrame(interest_over_time)\n",
    "            for query in df.iloc[0][\"values\"]:\n",
    "                query = query[\"query\"]\n",
    "                df[\"trend_\" + query] = df[\"values\"].apply(lambda x: next(x[i][\"value\"] for i in range(len(x)) if x[i][\"query\"] == query))\n",
    "                df[\"trend_\" + query] = df[\"trend_\" + query].apply(lambda x: x.replace('<', '').replace('>', ''))\n",
    "                df[\"trend_\" + query] = df[\"trend_\" + query].astype(int)\n",
    "\n",
    "            df.drop(columns=[\"values\", \"timestamp\"], inplace=True)\n",
    "            df[\"date_entree\"] = pd.to_datetime(df[\"date\"], format=\"%b %d, %Y\")\n",
    "            df.drop(columns=[\"date\"], inplace=True)\n",
    "            df.set_index(\"date_entree\", inplace=True)\n",
    "            all_dates_df = pd.concat([all_dates_df, df], axis=0)\n",
    "        except:\n",
    "            # Ce code permet de réessayer avec un batch plus petit mais cela ne résoud pas le problème, on décide donc de remplir de 0 plus tard\n",
    "            # On perd 270 jours à chaque fois mais cette erreur ne se produit généralement que quand il n'y a pas (peu?) de données\n",
    "\n",
    "            # if batch_size//2 > 1:\n",
    "            #     print(f\"On réessaye avec un batch plus petit de {batch_size//2} jours\")\n",
    "            #     try:\n",
    "            #         all_dates_df = pd.concat([all_dates_df, get_all_dates_df(bow, i, date, end_date, batch_size//2)], axis=0)\n",
    "            #     except:\n",
    "            #         print(f\"Problème avec {group} pour la période {date} - {end_date}\")\n",
    "            #         continue\n",
    "            # else:\n",
    "            #     print(f\"Problème avec {group} pour la période {date} - {end_date}\")\n",
    "            continue\n",
    "            \n",
    "\n",
    "    return all_dates_df\n",
    "\n",
    "\n",
    "# On récupère les tendance par lot de 5 mots max et 270 jours max\n",
    "idx = pd.date_range(START, STOP_DATA, freq='D')\n",
    "all_words_df = pd.DataFrame(index=idx)\n",
    "for i in range(0, len(bow), 5):\n",
    "    \n",
    "    all_dates_df = get_all_dates_df(bow, i, START, STOP_DATA, BATCH_SIZE)\n",
    "\n",
    "    # Fill missing dates\n",
    "    idx = pd.date_range(START, STOP_DATA, freq='D')\n",
    "    all_dates_df = all_dates_df.reset_index()\n",
    "\n",
    "    all_dates_df.fillna(0, inplace=True)\n",
    "    # all_dates_df.ffill(inplace=True)\n",
    "    # all_dates_df.bfill(inplace=True)\n",
    "\n",
    "    all_dates_df.set_index('date_entree', inplace=True)\n",
    "    all_words_df = pd.concat([all_words_df, all_dates_df], axis=1)\n",
    "\n",
    "all_words_df.reset_index(inplace=True)\n",
    "all_words_df.rename({\"index\": \"date_entree\"}, axis=1, inplace=True)\n",
    "\n",
    "# Tracer les graphiques\n",
    "for i in range(0, len(bow), 5):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    all_words_df.plot(x=\"date_entree\", y=all_words_df.loc[:, all_words_df.columns != \"date_entree\"].columns.tolist()[i:i + 5], title=\"Google Trends in France\", ax=ax)\n",
    "\n",
    "# On sauvegarde le dataframe\n",
    "all_words_df.to_csv('trends.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                        \n",
    "from pytrends.request import TrendReq\n",
    "pytrend = TrendReq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "The request failed: Google returned a response with code 400",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pytrend\u001b[38;5;241m.\u001b[39mbuild_payload(kw_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpizza\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbagel\u001b[39m\u001b[38;5;124m'\u001b[39m], timeframe\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-09-01 2022-09-02\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-09-01 2022-09-02\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpytrend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultirange_interest_over_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/WORKSPACES/ARS_Project/.venv/lib/python3.10/site-packages/pytrends/request.py:285\u001b[0m, in \u001b[0;36mTrendReq.multirange_interest_over_time\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m over_time_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# convert to string as requests will mangle\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterest_over_time_widget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterest_over_time_widget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtz\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtz\n\u001b[1;32m    282\u001b[0m }\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# make the request and parse the returned json\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m req_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMULTIRANGE_INTEREST_OVER_TIME_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_METHOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrim_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mover_time_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(req_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimelineData\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (df\u001b[38;5;241m.\u001b[39mempty):\n",
      "File \u001b[0;32m~/Documents/WORKSPACES/ARS_Project/.venv/lib/python3.10/site-packages/pytrends/request.py:160\u001b[0m, in \u001b[0;36mTrendReq._get_data\u001b[0;34m(self, url, method, trim_chars, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m status_codes\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mtoo_many_requests:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTooManyRequestsError\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mResponseError\u001b[38;5;241m.\u001b[39mfrom_response(response)\n",
      "\u001b[0;31mResponseError\u001b[0m: The request failed: Google returned a response with code 400"
     ]
    }
   ],
   "source": [
    "# Attention au ban de Google\n",
    "pytrend.build_payload(kw_list=['pizza', 'bagel'], timeframe=['2022-09-01 2022-09-02', '2022-09-01 2022-09-02'])\n",
    "pytrend.multirange_interest_over_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apify_client import ApifyClient\n",
    "\n",
    "client = ApifyClient('apify_api_czHu0VeyZoxP9fEApsHUDLJwmKzf0e01ymiK')\n",
    "# Prepare the Actor input\n",
    "\n",
    "run_input = {\n",
    "    \"searchTerms\": [\"webscraping\"],\n",
    "    \"isMultiple\": False,\n",
    "    \"timeRange\": \"\",\n",
    "    \"geo\": \"\",\n",
    "    \"viewedFrom\": \"\",\n",
    "    \"skipDebugScreen\": False,\n",
    "    \"isPublic\": False,\n",
    "    \"category\": \"\",\n",
    "    \"maxItems\": 0,\n",
    "    \"maxConcurrency\": 10,\n",
    "    \"maxRequestRetries\": 7,\n",
    "    \"pageLoadTimeoutSecs\": 180,\n",
    "}\n",
    "\n",
    "\n",
    "# Run the Actor and wait for it to finish\n",
    "\n",
    "run = client.actor(\"DyNQEYDj9awfGQf9A\").call(run_input=run_input)\n",
    "\n",
    "\n",
    "# Fetch and print Actor results from the run's dataset (if there are any)\n",
    "\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
