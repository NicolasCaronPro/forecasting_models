{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d101ce7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bb0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\n",
    "    os.path.join(os.path.dirname('__file__'), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ff4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtla import NixtlaClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nixtla_client = NixtlaClient(\n",
    "    api_key = 'my_api_key_provided_by_nixtla'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0dd66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nixtla_client.validate_api_key()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Error getting driver and runtime versions:\n",
      "\n",
      "stdout:\n",
      "\n",
      "\n",
      "\n",
      "stderr:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/maxime/Documents/WORKSPACES/forecasting_models/.venv/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 254, in ensure_initialized\n",
      "    self.cuInit(0)\n",
      "  File \"/home/maxime/Documents/WORKSPACES/forecasting_models/.venv/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 327, in safe_cuda_api_call\n",
      "    self._check_ctypes_error(fname, retcode)\n",
      "  File \"/home/maxime/Documents/WORKSPACES/forecasting_models/.venv/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 395, in _check_ctypes_error\n",
      "    raise CudaAPIError(retcode, msg)\n",
      "numba.cuda.cudadrv.driver.CudaAPIError: [999] Call to cuInit results in CUDA_ERROR_UNKNOWN\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 4, in <module>\n",
      "  File \"/home/maxime/Documents/WORKSPACES/forecasting_models/.venv/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 292, in __getattr__\n",
      "    self.ensure_initialized()\n",
      "  File \"/home/maxime/Documents/WORKSPACES/forecasting_models/.venv/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 258, in ensure_initialized\n",
      "    raise CudaSupportError(f\"Error at driver init: {description}\")\n",
      "numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: Call to cuInit results in CUDA_ERROR_UNKNOWN (999)\n",
      "\n",
      "\n",
      "Not patching Numba\n"
     ]
    },
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorUnknown: unknown error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCUDARuntimeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn_api_models_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_tabular_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTabularDataset\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_experiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseExperiment\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mft\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/WORKSPACES/forecasting_models/src/experiments/base_experiment.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcd\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     USE_CUDA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/WORKSPACES/forecasting_models/.venv/lib/python3.10/site-packages/cudf/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcudf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpu_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_setup\n\u001b[1;32m      9\u001b[0m _setup_numba()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mvalidate_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m numba_config, cuda\n",
      "File \u001b[0;32m~/Documents/WORKSPACES/forecasting_models/.venv/lib/python3.10/site-packages/cudf/utils/gpu_utils.py:55\u001b[0m, in \u001b[0;36mvalidate_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CUDARuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01min\u001b[39;00m notify_caller_errors:\n\u001b[0;32m---> 55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# If there is no GPU detected, set `gpus_count` to -1\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     gpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/WORKSPACES/forecasting_models/.venv/lib/python3.10/site-packages/cudf/utils/gpu_utils.py:52\u001b[0m, in \u001b[0;36mvalidate_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m notify_caller_errors \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     cudaError_t\u001b[38;5;241m.\u001b[39mcudaErrorInitializationError,\n\u001b[1;32m     33\u001b[0m     cudaError_t\u001b[38;5;241m.\u001b[39mcudaErrorInsufficientDriver,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     cudaError_t\u001b[38;5;241m.\u001b[39mcudaErrorApiFailureBase,\n\u001b[1;32m     49\u001b[0m }\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     gpus_count \u001b[38;5;241m=\u001b[39m \u001b[43mgetDeviceCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CUDARuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01min\u001b[39;00m notify_caller_errors:\n",
      "File \u001b[0;32m~/Documents/WORKSPACES/forecasting_models/.venv/lib/python3.10/site-packages/rmm/_cuda/gpu.py:102\u001b[0m, in \u001b[0;36mgetDeviceCount\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m status, count \u001b[38;5;241m=\u001b[39m cudart\u001b[38;5;241m.\u001b[39mcudaGetDeviceCount()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m cudart\u001b[38;5;241m.\u001b[39mcudaError_t\u001b[38;5;241m.\u001b[39mcudaSuccess:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CUDARuntimeError(status)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m count\n",
      "\u001b[0;31mCUDARuntimeError\u001b[0m: cudaErrorUnknown: unknown error"
     ]
    }
   ],
   "source": [
    "from src.encoding.encoders import *\n",
    "from src.encoding.tools import create_encoding_pipeline\n",
    "from src.models.sklearn_api_model import ModelStacking, ModelVoting\n",
    "from src.models.sklearn_api_models_config import get_model\n",
    "from src.datasets.base_tabular_dataset import BaseTabularDataset\n",
    "from src.experiments.base_experiment import BaseExperiment\n",
    "import src.features as ft\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "from typing import List, Union, Optional\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from src.datasets.base_tabular_dataset import BaseTabularDataset\n",
    "from src.encoding.tools import create_encoding_pipeline\n",
    "from src.experiments.features_selection import get_features, explore_features\n",
    "from src.models.sklearn_api_model import Model, ModelTree\n",
    "import src.features as ft\n",
    "import mlflow.sklearn\n",
    "import mlflow\n",
    "import mlflow.data.pandas_dataset\n",
    "from mlflow.models import infer_signature\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "import cudf as cd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a0e4f",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logger used by all modules\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO, encoding=\"utf-8\",\n",
    "                    format=\"%(name)s %(asctime)s: %(levelname)s: %(message)s\", handlers=[logging.StreamHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0710aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory of the project\n",
    "root_dir = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "root_dir = pathlib.Path(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bacf51",
   "metadata": {},
   "source": [
    "##### Encoding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an encoding scheme to create the encoding pipeline\n",
    "encoders_dict = {\n",
    "    'number': {\n",
    "        'as_number': {\n",
    "            'imputers': [imputers.SimpleImputer(strategy='mean')],\n",
    "            'encoders': [\n",
    "                ne.StandardScaler(),\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'category': {\n",
    "        'as_category': {\n",
    "            'imputers': [imputers.SimpleImputer(strategy='most_frequent')],\n",
    "            'encoders': [\n",
    "                ne.MultiTargetEncoder(drop_invariant=True, return_df=True),\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'datetime': {\n",
    "        'as_number': {\n",
    "            'imputers': [de.DateFeatureExtractor()],\n",
    "            'encoders': [\n",
    "                ne.CyclicalFeatures(drop_original=True)\n",
    "            ]\n",
    "        },\n",
    "        'as_category': {\n",
    "            'imputers': [de.DateFeatureExtractor(dtype='category')],\n",
    "            'encoders': [\n",
    "                ne.MultiTargetEncoder(drop_invariant=True, return_df=True),\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'boolean': {\n",
    "        'as_number': {\n",
    "            'imputers': [imputers.SimpleImputer(strategy='most_frequent')],\n",
    "             'encoders': [ne.BooleanEncoder()]\n",
    "         }\n",
    "\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429854cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoding pipeline\n",
    "pipeline = create_encoding_pipeline(encoders_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae03d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62319382",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611419a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for the fetching of the data\n",
    "fetch_config = {\n",
    "    \"data_start\": '01-01-2019',\n",
    "    \"data_stop\": '31-12-2023',\n",
    "    'data_dir': root_dir / 'data',\n",
    "    # \"locations\": ['CHU Dijon', 'CH Beaune', 'CH Semur', 'CH Chatillon Montbard', 'CH privé Dijon', 'CH Langres', 'CH Chaumont', 'HNFC', 'CHU Besançon']\n",
    "    \"locations\": ['CHU Dijon']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used in the dataset\n",
    "ars_features_class = [\n",
    "    ft.HospitalFeatures,\n",
    "    ft.AirQualityFeatures,\n",
    "    ft.EpidemiologicalFeatures,\n",
    "    # ft.FireFightersFeatures(include_calls=False),\n",
    "    ft.GoogleTrendFeatures,\n",
    "    ft.MeteorologicalFeatures,\n",
    "    ft.SociologicalFeatures,\n",
    "    ft.SportsCompetitionFeatures,\n",
    "    ft.TrafficFeatures\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration of the dataset\n",
    "get_dataset_config = {\n",
    "    'from_date': '01-01-2019',\n",
    "    'to_date': '30-12-2023',\n",
    "    'locations': ['CHU Dijon'],\n",
    "    # 'locations': ['CHU Dijon', 'CH Beaune', 'CH Semur', 'CH Chatillon Montbard', 'CH privé Dijon'],\n",
    "    # 'axis': 'rows',\n",
    "    'shift': range(1, 8, 1),\n",
    "    # 'rolling_window': [7, 14, 31, 365],\n",
    "    'freq': '1D',\n",
    "    'split_config': {'test_size': 0.2, 'val_size': None, 'shuffle': False},\n",
    "    'create_X_y': True,\n",
    "    'encoding_pipeline': pipeline,\n",
    "    'targets_names': ['nb_vers_hospit'],\n",
    "    'targets_shift': -3,\n",
    "    'targets_rolling_window': 3,\n",
    "    'targets_history_shifts': range(1, 8, 1),\n",
    "    # 'targets_history_rolling_windows': [7, 14, 31, 365],\n",
    "    'targets_locations': ['CHU Dijon'],\n",
    "    'drop_constant_thr': 1.0,\n",
    "    'data_dir': root_dir / 'data'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1312a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and fetch the data from the source then call get_dataset() method to fill the different attributes (X and y) of the different sets, and their encodings\n",
    "arsTabularDataset = BaseTabularDataset(features_classes=ars_features_class, logger=logger, fetch_config=fetch_config, getter_config=get_dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29da147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = arsTabularDataset.data\n",
    "\n",
    "# # Identifie les colonnes contenant des NaN\n",
    "# cols_with_nan = df.columns[df.isna().any()].tolist()\n",
    "# print(\"Colonnes contenant des NaN:\", cols_with_nan)\n",
    "\n",
    "# # Affiche la liste des index des lignes contenant des NaN pour chaque colonne\n",
    "# nan_indices = {col: df[df[col].isna()].index.tolist() for col in cols_with_nan}\n",
    "# nan_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e24100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(fetch_config['data_dir'] / f'datasets/full_dataset_{get_dataset_config['locations']}.csv')\n",
    "# df_target = arsTabularDataset.data[arsTabularDataset.targets_names]\n",
    "# df_target.to_csv(fetch_config['data_dir'] / f'datasets/full_dataset_{get_dataset_config['locations']}_targets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ace063",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(arsTabularDataset.data.columns.to_list())\n",
    "# Define the model parameters\n",
    "from src.models.obectives import *\n",
    "model_params = {\n",
    "    # 'tree_method': 'approx',\n",
    "    # 'tree_method': 'hist',\n",
    "    'early_stopping_rounds': 10,\n",
    "    # 'eval_set': [(arsTabularDataset.enc_X_val, arsTabularDataset.y_val)], # TODO: to be set in the experiment's run method\n",
    "    'verbosity': 1,\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': 0.1,\n",
    "    'min_child_weight': 5,\n",
    "    'subsample': 0.5, \n",
    "    'sampling_method': 'gradient_based',\n",
    "    'colsample_bytree':0.5,\n",
    "    'colsample_bylevel':0.5,\n",
    "    'colsample_bynode':0.5,\n",
    "    # 'huber_slope': 1.0,\n",
    "    # 'quantile_alpha': np.array([0.5]),\n",
    "    'objective': 'reg:squarederror',\n",
    "    # 'tweedie_variance_power': 2,\n",
    "    # 'alpha': 10\n",
    "    # 'multi_strategy': 'one_output_per_tree',\n",
    "    # 'multi_strategy': 'multi_output_tree' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['mae', 'mse', 'rmse', 'w_rmse', 'pw_rmse', 'msle', 'rmsle', 'r2', 'mqe', 'msse', 'max_error', 'explained_variance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e1d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = get_model(model_type='xgboost',\n",
    "                  name='exogeneous_regressor',\n",
    "                  device='cuda',\n",
    "                  task_type='regression',\n",
    "                  test_metrics=metrics,\n",
    "                  params=model_params,\n",
    "                  n=2,\n",
    "                  ensemble_method='voting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0131aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# endogeneous_model = copy.deepcopy(exogenous_model)\n",
    "# endogeneous_model.set_params({'name':'endogeneous_regressor'})\n",
    "# model = ModelVoting(models=[exogenous_model, endogeneous_model], loss=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e243fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "ars_experiment = BaseExperiment(logger=logger, dataset=arsTabularDataset, model=model, name=', '.join(arsTabularDataset.targets_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model fitting config\n",
    "grid_params = {\n",
    "    'max_depth': [3, 4, 5, 7, 9],\n",
    "    'gamma' : [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86be32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'verbose': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1850382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c25cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config={\"optimization\": \"grid\", \"grid_params\": grid_params, \"fit_params\": fit_params, 'cv_folds': TimeSeriesSplit()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b10940b",
   "metadata": {},
   "source": [
    "### Start run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae681f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = mlflow.start_run(run_name='run_' + str(ars_experiment.run_nb), log_system_metrics=True)\n",
    "run_dir = ars_experiment.dir_runs / f'{run.info.run_id}/artifacts/'\n",
    "run_dir = pathlib.Path(run_dir)\n",
    "ars_experiment.logger.info(\"Running the experiment...\")\n",
    "# ars_experiment.dataset.get_dataset(**get_dataset_config)\n",
    "# Certain fit_params doivent être initialisés après la création des datasets : eval_set\n",
    "model_config['fit_params'].update({'eval_set': [((ars_experiment.dataset.enc_X_train, ars_experiment.dataset.y_train[target]), (\n",
    "    ars_experiment.dataset.enc_X_val, ars_experiment.dataset.y_val[target])) for target in ars_experiment.dataset.targets_names]})\n",
    "\n",
    "mlflow.log_table(data=ars_experiment.dataset.data,\n",
    "                artifact_file='datasets/full_dataset.json')\n",
    "if find_best_features:\n",
    "    if isinstance(find_best_features, bool):\n",
    "        # selected_features = ars_experiment.get_important_features(dataset=dataset, model=ars_experiment.model, model_config=model_config)\n",
    "        selected_features = ['nb_emmergencies%%J-7', 'nb_emmergencies%%J-1', 'nb_emmergencies%%J-2','nb_emmergencies%%J-3',\n",
    "                            'nb_emmergencies', 'NO2_FR26094%%mean_7J', 'nb_emmergencies%%mean_365J', 'eveBankHolidays',\n",
    "                            'meteo_wdir%%J-7', 'confinement1', 'trend_grippe%%mean_7J', 'trend_hopital%%J-3', 'trend_vaccin%%J-2',\n",
    "                            'inc_diarrhee%%J-7', 'PM25_FR26094%%J-7', 'trend_crampes abdominales%%J-7', 'trend_médecin',\n",
    "                            'trend_crampes abdominales%%mean_7J', 'confinement2', 'NO2_FR26010', 'trend_hopital%%J-2', 'trend_mal de tête%%mean_7J',\n",
    "                            'trend_paralysie%%J-7', 'trend_accident de voiture%%mean_7J', 'trend_paralysie%%mean_7J', 'meteo_tavg%%mean_7J',\n",
    "                            'trend_insuffisance cardiaque', 'trend_fièvre%%J-7', 'trend_infection respiratoire%%mean_7J']\n",
    "        selected_features.extend(['PM10_FR26005%%mean_31J', 'foot%%std_14J', 'inc_ira%%mean_31J', \n",
    "                            'meteo_tmin%%mean_31J', 'trend_vaccin%%mean_31J', 'confinement2',\n",
    "                            'meteo_tmax%%mean_31J', 'after_HNFC_moving', 'trend_vaccin%%mean_14J',\n",
    "                            'trend_hopital%%mean_31J', 'trend_hopital%%mean_14J', 'date##week_cos',\n",
    "                            'O3_FR26010%%mean_31J', 'O3_FR26005%%mean_31J', 'meteo_tavg%%mean_31J',\n",
    "                            'inc_grippe%%mean_31J', 'inc_grippe%%mean_14J', 'date##week_sin',\n",
    "                            'date##dayofYear_sin', 'confinement1'])\n",
    "        selected_features = ['nb_emmergencies_CHU Dijon', 'nb_emmergencies_CHU Dijon%%J-1',\n",
    "        'nb_emmergencies_CHU Dijon%%J-2', 'nb_emmergencies_CHU Dijon%%J-3',\n",
    "        'nb_emmergencies_CHU Dijon%%J-4', 'nb_emmergencies_CHU Dijon%%J-5',\n",
    "        'nb_emmergencies_CHU Dijon%%J-6', 'nb_emmergencies_CHU Dijon%%J-7',\n",
    "        'nb_emmergencies_CHU Dijon%%J-8', 'nb_emmergencies_CHU Dijon%%J-9',\n",
    "        'nb_emmergencies_CHU Dijon%%J-10', 'nb_emmergencies_CHU Dijon%%J-11',\n",
    "        'nb_emmergencies_CHU Dijon%%J-12', 'nb_emmergencies_CHU Dijon%%J-13',\n",
    "        'nb_emmergencies_CHU Dijon%%mean_7J',\n",
    "        'nb_emmergencies_CHU Dijon%%mean_14J', 'inc_diarrhee', 'inc_ira',\n",
    "        'inc_diarrhee%%J-1', 'inc_diarrhee%%J-2', 'inc_diarrhee%%J-3',\n",
    "        'inc_ira%%J-1', 'inc_ira%%J-2', 'inc_ira%%J-3', 'inc_ira%%J-4',\n",
    "        'inc_ira%%J-5', 'inc_ira%%J-6', 'inc_ira%%J-8', 'inc_ira%%J-9',\n",
    "        'inc_ira%%J-10', 'inc_ira%%J-11', 'inc_ira%%J-12', 'inc_ira%%J-13',\n",
    "        'target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J%%J-6',\n",
    "        'target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J%%J-7',\n",
    "        'target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J%%J-8',\n",
    "        'target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J%%J-9',\n",
    "        'target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J%%J-10',\n",
    "        'target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J%%J-11',\n",
    "        'target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J%%J-12',\n",
    "        'target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J%%J-13',\n",
    "        'target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J%%mean_7J%%J-6',\n",
    "        'target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J%%mean_14J%%J-6',\n",
    "        'date##week##cat##target_nb_emmergencies_CHU Dijon%%J+3%%mean_3J']\n",
    "    elif isinstance(find_best_features, List):\n",
    "        selected_features = find_best_features\n",
    "    elif isinstance(find_best_features, str):\n",
    "        selected_features = [find_best_features]\n",
    "    else:\n",
    "        raise ValueError('find_best_features must be a bool, a list or a string')\n",
    "\n",
    "    if get_dataset_config['axis'] == 'columns':\n",
    "        for loc in get_dataset_config['locations']:\n",
    "            selected_features = [feat + '_' + loc.name if not feat.endswith(loc.name) else feat for feat in selected_features]\n",
    "    elif get_dataset_config['axis'] == 'rows' and 'location' not in ars_experiment.dataset.columns.to_list():\n",
    "        selected_features.append('location')\n",
    "\n",
    "    # selected_features = dataset.enc_X_train.columns.to_list()\n",
    "    get_dataset_config['features_names'] = selected_features\n",
    "    ars_experiment.logger.info(\n",
    "        'Features selected: {}'.format(selected_features))\n",
    "    ars_experiment.dataset.get_dataset(**get_dataset_config)\n",
    "    mlflow.log_table(data=ars_experiment.dataset.data,\n",
    "                    artifact_file='datasets/full_dataset_feature_selection.json')\n",
    "    model_config['fit_params']['eval_set'] = [\n",
    "        (ars_experiment.dataset.enc_X_val, ars_experiment.dataset.y_val[target]) for target in ars_experiment.dataset.targets_names]\n",
    "\n",
    "mlflow.log_table(data=ars_experiment.dataset.train_set,\n",
    "                    artifact_file='datasets/train_set.json')\n",
    "mlflow.log_table(data=ars_experiment.dataset.val_set,\n",
    "                    artifact_file='datasets/val_set.json')\n",
    "mlflow.log_table(data=ars_experiment.dataset.test_set,\n",
    "                    artifact_file='datasets/test_set.json')\n",
    "\n",
    "train_dataset = mlflow.data.pandas_dataset.from_pandas(\n",
    "    ars_experiment.dataset.train_set)\n",
    "val_dataset = mlflow.data.pandas_dataset.from_pandas(\n",
    "    ars_experiment.dataset.val_set)\n",
    "test_dataset = mlflow.data.pandas_dataset.from_pandas(\n",
    "    ars_experiment.dataset.test_set)\n",
    "\n",
    "mlflow.log_input(dataset=train_dataset, context='training')\n",
    "mlflow.log_input(dataset=val_dataset, context='validation')\n",
    "mlflow.log_input(dataset=test_dataset, context='testing')\n",
    "\n",
    "dataset_config_log = get_dataset_config.copy()\n",
    "dataset_config_log['locations'] = [loc.name for loc in dataset_config_log.pop('locations')]\n",
    "dataset_config_log['targets_locations'] = [loc.name for loc in dataset_config_log.pop('targets_locations')]\n",
    "mlflow.log_params(dataset_config_log)\n",
    "mlflow.log_params({f'grid_{key}': value for key,\n",
    "                    value in model_config['grid_params'].items()})\n",
    "# mlflow.log_params(model_config['params'])\n",
    "mlflow.log_params(model_config['fit_params'])\n",
    "mlflow.log_param('optimization', model_config['optimization'])\n",
    "ars_experiment.model.fit(cd.DataFrame(ars_experiment.dataset.enc_X_train),\n",
    "                ars_experiment.dataset.y_train, **model_config)\n",
    "ars_experiment.logger.info(\"Model fitted.\")\n",
    "params = ars_experiment.model.get_params(deep=True)\n",
    "if params['objective'] is not None:\n",
    "    # Check if objective is a function\n",
    "    if callable(params['objective']):\n",
    "        params['objective'] = params['objective'].__name__\n",
    "\n",
    "if params['eval_metric'] is not None:\n",
    "    if callable(params['eval_metric']):\n",
    "        params['eval_metric'] = params['eval_metric'].__name__\n",
    "    else:\n",
    "        params['eval_metric'] = params['eval_metric']\n",
    "mlflow.log_params(params=params)\n",
    "y_pred = ars_experiment.predict(ars_experiment.dataset)\n",
    "mlflow.log_table(data=y_pred, artifact_file='datasets/pred.json')\n",
    "scores = ars_experiment.score(ars_experiment.dataset)\n",
    "mlflow.log_metrics(scores)\n",
    "print(scores)\n",
    "signature = infer_signature(ars_experiment.dataset.enc_X_test, y_pred)\n",
    "mlflow.sklearn.log_model(ars_experiment.model, \"model\", signature=signature)\n",
    "figure, ax = ars_experiment.plot(ars_experiment.dataset, y_pred, scores)\n",
    "mlflow.log_figure(figure, 'predictions.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_fig = ars_experiment.model.get_prediction_error_display(y=ars_experiment.dataset.y_test, y_pred=y_pred)\n",
    "mlflow.log_figure(error_fig, 'errors.png')\n",
    "\n",
    "ars_experiment.run_nb += 1\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: Model = ars_experiment.model\n",
    "best_estimator: xgb.XGBModel = model.best_estimator_\n",
    "booster: xgb.Booster = best_estimator.get_booster()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e81fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Parameters:\")\n",
    "for param in best_estimator.get_xgb_params().keys():\n",
    "    print(f\"{param}: {best_estimator.get_xgb_params()[param]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1003405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFeature Importances:\")\n",
    "print(best_estimator.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(best_estimator, importance_type='cover', max_num_features=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc98a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un exemple pour déterminer l'importance des variables après un pré-apprentissage XGBoost\n",
    "importance_gain = booster.get_score(importance_type='gain')\n",
    "importance_cover = booster.get_score(importance_type='cover')\n",
    "importance_weight = booster.get_score(importance_type='weight')\n",
    "\n",
    "df_gain = pd.DataFrame.from_dict(importance_gain, orient='index', columns=['gain'])\n",
    "df_cover = pd.DataFrame.from_dict(importance_cover, orient='index', columns=['cover'])\n",
    "df_weight = pd.DataFrame.from_dict(importance_weight, orient='index', columns=['weight'])\n",
    "\n",
    "df = df_gain.join(df_cover, how='outer').join(df_weight, how='outer')\n",
    "df.fillna(0, inplace=True)  # Remplacer les valeurs manquantes par 0 si nécessaire\n",
    "\n",
    "df = df_gain.join(df_cover, how='outer').join(df_weight, how='outer')\n",
    "df.fillna(0, inplace=True)  # Remplacer les valeurs manquantes par 0 si nécessaire\n",
    "\n",
    "df['gain_norm'] = df['gain'] / df['gain'].sum()\n",
    "df['cover_norm'] = df['cover'] / df['cover'].sum()\n",
    "df['weight_norm'] = df['weight'] / df['weight'].sum()\n",
    "\n",
    "w_gain = 0.5\n",
    "w_cover = 0.3\n",
    "w_weight = 0.2\n",
    "\n",
    "df['importance'] = (df['gain_norm'] * w_gain) + (df['cover_norm'] * w_cover) + (df['weight_norm'] * w_weight)\n",
    "\n",
    "df.sort_values(by='importance', ascending=False, inplace=True)\n",
    "df['rank'] = df['importance'].rank(ascending=False)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'feature'}, inplace=True)\n",
    "# print(df[['feature', 'gain', 'cover', 'weight', 'importance', 'rank']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae273322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features by importance\n",
    "df_sorted = df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# # Select only the 30 first features (based on sorted order)\n",
    "# df_sorted = df_sorted.nlargest(30, 'importance')\n",
    "\n",
    "# Plotting the horizontal bar chart\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.barh(df_sorted['feature'], df_sorted['importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "# Adding the importance values next to each bar\n",
    "for index, value in enumerate(df_sorted['importance']):\n",
    "    plt.text(value + 0.0005, index, f'{value:.5f}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ebf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.to_csv(\"feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adba412",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ars_experiment.dataset.enc_X_train\n",
    "X_test = ars_experiment.dataset.enc_X_test\n",
    "X_val = ars_experiment.dataset.enc_X_val\n",
    "\n",
    "y_train = ars_experiment.dataset.y_train\n",
    "y_test = ars_experiment.dataset.y_test\n",
    "y_val = ars_experiment.dataset.y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34549e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_prediction(dataset: BaseTabularDataset):\n",
    "    predictions = pd.DataFrame(index=dataset.y_test.index)\n",
    "    predictions['preds'] = np.nan\n",
    "    for i in predictions.index[6:]:\n",
    "        df_copy = dataset.enc_X_test\n",
    "        # print(f'{i}')\n",
    "\n",
    "        # Remplacer les valeurs réelles par les valeurs prédites\n",
    "        for j in range(6, 0, -1):\n",
    "            # print(f'on prédit J-{j} et on place cette prédiction dans les colonnes d\\'historique des jours d\\'historiques suivant')\n",
    "            # print(pd.DataFrame(df_copy.loc[i - dt.timedelta(days=j)]).T)\n",
    "            prediction = model.predict(pd.DataFrame(df_copy.loc[i - dt.timedelta(days=j)]).T)[0]\n",
    "            # print(prediction)\n",
    "            is_last_history_day = True\n",
    "            for k in range(j-1, 0, -1):\n",
    "                # print(f'on place la prédiction de J-{j} dans timeserie_J-{j-k} de la ligne J-{k}')\n",
    "                df_copy.loc[i - dt.timedelta(days=k), f'target_nb_vers_hospit%J+3%mean_3J%%J-{j-k}'] = prediction\n",
    "                is_last_history_day = False\n",
    "            if is_last_history_day:\n",
    "                print(f'prediction pour le {i}: {prediction}')\n",
    "                predictions.loc[i, 'preds'] = prediction\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = recursive_prediction(ars_experiment.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21955d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ars_experiment.plot(ars_experiment.dataset, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db9758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer = explainer_xgb = shap.TreeExplainer(best_estimator)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values=shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(explainer_xgb.expected_value, shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "# ars_experiment.run(dataset_config=get_dataset_config, model_config=model_config, find_best_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = ars_experiment.predict(ars_experiment.dataset)\n",
    "# y_true = ars_experiment.dataset.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f239dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ars_experiment.model.get_prediction_error_display(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv(\"X_train.csv\", index=False)\n",
    "# X_test.to_csv(\"X_test.csv\", index=False)\n",
    "# X_val.to_csv(\"X_val.csv\", index=False)\n",
    "# y_train.to_csv(\"y_train.csv\", index=False)\n",
    "# y_test.to_csv(\"y_test.csv\", index=False)\n",
    "# y_val.to_csv(\"y_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ars_experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = arsTabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.enc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd40db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = dataset.enc_data\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.cluster import hierarchy\n",
    "# from scipy.spatial.distance import squareform\n",
    "# from scipy.stats import spearmanr\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# corr = spearmanr(X).correlation\n",
    "\n",
    "# # Ensure the correlation matrix is symmetric\n",
    "# corr = (corr + corr.T) / 2\n",
    "# np.fill_diagonal(corr, 1)\n",
    "\n",
    "# # We convert the correlation matrix to a distance matrix before performing\n",
    "# # hierarchical clustering using Ward's linkage.\n",
    "# distance_matrix = 1 - np.abs(corr)\n",
    "# dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "# dendro = hierarchy.dendrogram(\n",
    "#     dist_linkage, labels=X.columns.to_list(), ax=ax1, leaf_rotation=90\n",
    "# )\n",
    "# dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "# ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "# ax2.set_xticks(dendro_idx)\n",
    "# ax2.set_yticks(dendro_idx)\n",
    "# ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "# ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shap_select import shap_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3609cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features_df = shap_select(model.best_estimator_, X_val, y_val, task=\"regression\", threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec14b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba18872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bst = model.best_estimator_.get_booster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64929a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_gain = bst.get_score(importance_type='gain')\n",
    "# importance_cover = bst.get_score(importance_type='cover')\n",
    "# importance_weight = bst.get_score(importance_type='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cover = pd.DataFrame(importance_cover, index=[0]).T\n",
    "# df_gain = pd.DataFrame(importance_gain, index=[0]).T\n",
    "# df_weight = pd.DataFrame(importance_weight, index=[0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cover.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cover.sort_values(by=0).plot(kind=\"barh\", figsize=(15,20))\n",
    "# df_gain.sort_values(by=0).plot(kind=\"barh\", figsize=(15,20))\n",
    "# df_weight.sort_values(by=0).plot(kind=\"barh\", figsize=(15,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.shapley_additive_explanation(X_test, outname='shap_b', dir_output='.', mode=\"beeswarm\", figsize=(50, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19133e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "# cluster_id_to_feature_ids = defaultdict(list)\n",
    "# for idx, cluster_id in enumerate(cluster_ids):\n",
    "#     cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "# selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "# selected_features_names = X.columns[selected_features]\n",
    "# X_train_sel = X_train[selected_features_names]\n",
    "# X_test_sel = X_test[selected_features_names]\n",
    "# X_val_sel = X_val[selected_features_names]\n",
    "# model_config['fit_params'].update({'eval_set': [(X_val_sel, y_val[target]) for target in ars_experiment.dataset.targets_names]})\n",
    "# model.fit(X_train_sel, y_train, **model_config)\n",
    "# print(\n",
    "#     \"Baseline accuracy on test data with features removed:\"\n",
    "#     f\" {model.score(X_test_sel, y_test):.2}\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808aacff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# from sklearn.utils.fixes import parse_version\n",
    "\n",
    "\n",
    "# def plot_permutation_importance(clf, X, y, ax):\n",
    "#     result = permutation_importance(clf, X, y, n_repeats=10, random_state=42)\n",
    "#     perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "#     # `labels` argument in boxplot is deprecated in matplotlib 3.9 and has been\n",
    "#     # renamed to `tick_labels`. The following code handles this.\n",
    "#     tick_labels_parameter_name = (\n",
    "#         \"tick_labels\"\n",
    "#         if parse_version(matplotlib.__version__) >= parse_version(\"3.9\")\n",
    "#         else \"labels\"\n",
    "#     )\n",
    "#     tick_labels_dict = {tick_labels_parameter_name: X.columns[perm_sorted_idx]}\n",
    "#     ax.boxplot(result.importances[perm_sorted_idx].T, vert=False, **tick_labels_dict)\n",
    "#     ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "#     return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f729a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(7, 6))\n",
    "# plot_permutation_importance(model, X_test_sel, y_test, ax)\n",
    "# ax.set_title(\"Permutation Importances on selected subset of features\\n(test set)\")\n",
    "# ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "# ax.figure.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first line of x train\n",
    "# X_train_sel[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0968b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1745191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(X_train_sel[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92292f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "# shap.initjs()\n",
    "# explainer_xgb = shap.TreeExplainer(model.best_estimator_)\n",
    "# single_explanation = explainer_xgb.shap_values(X_train_sel[:1])\n",
    "# shap.summary_plot(single_explanation, X_test_sel, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ars_experiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c141efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = ars_experiment.dataset.enc_X_train\n",
    "# X_test = ars_experiment.dataset.enc_X_test\n",
    "# X_val = ars_experiment.dataset.enc_X_val\n",
    "\n",
    "# y_train = ars_experiment.dataset.y_train\n",
    "# y_test = ars_experiment.dataset.y_test\n",
    "# y_val = ars_experiment.dataset.y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer_xgb = shap.TreeExplainer(model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_explanation = explainer_xgb.shap_values(X_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4af7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_values_xgb = explainer_xgb.shap_values(X_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b71ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.dependence_plot('inc_grippe', shap_values_xgb, X_train_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_values_xgb"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
