{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bb0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\n",
    "    os.path.join(os.path.dirname('__file__'), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ff4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c264361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.encoding.encoders import *\n",
    "from src.encoding.tools import create_encoding_pipeline\n",
    "from src.models.sklearn_models import save_object, Model\n",
    "from src.models.sklearn_models_config import get_model\n",
    "from src.datasets.base_tabular_dataset import BaseTabularDataset\n",
    "from src.experiments.base_experiment import BaseExperiment\n",
    "import src.features as ft\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0bd56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logger used by all modules\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO, encoding=\"utf-8\",\n",
    "                    format=\"%(name)s %(asctime)s: %(levelname)s: %(message)s\", handlers=[logging.StreamHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0710aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory of the project\n",
    "root_dir = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "root_dir = pathlib.Path(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611419a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for the fetching of the data\n",
    "fetch_config = {\n",
    "    \"data_start\": '01-01-2017',\n",
    "    \"data_stop\": '31-12-2023',\n",
    "    'data_dir': root_dir / 'data',\n",
    "    \"etablissement\": \"CHU Dijon\",\n",
    "    \"departement\": \"21\",\n",
    "    'region': 'BOURGOGNE'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a951beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to be used in the dataset\n",
    "ars_features_class = [\n",
    "    ft.HopitalFeatures,\n",
    "    ft.AirQualityFeatures,\n",
    "    ft.EpidemiologicalFeatures,\n",
    "    # ft.FireFightersFeatures(include_calls=False),\n",
    "    ft.GoogleTrendFeatures,\n",
    "    ft.MeteorologicalFeatures,\n",
    "    ft.SociologicalFeatures,\n",
    "    ft.SportsCompetitionFeatures,\n",
    "    ft.TrafficFeatures\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29491d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the target columns to be predicted\n",
    "target_colomns = ['nb_emmergencies_CHU Dijon']\n",
    "# target_colomns = ['nb_vers_hospit']\n",
    "# target_colomns = ['nb_hospit_np_adults%%J+1%%mean_7J']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d1313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an encoding scheme to create the encoding pipeline\n",
    "encoders_dict = {\n",
    "    'number': {\n",
    "        'as_number': {\n",
    "            'imputers': [imputers.SimpleImputer(strategy='mean')],\n",
    "            'encoders': [\n",
    "                ne.StandardScaler(),\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'category': {\n",
    "        'as_category': {\n",
    "            'imputers': [imputers.SimpleImputer(strategy='most_frequent')],\n",
    "            'encoders': [\n",
    "                ne.MultiTargetEncoder(drop_invariant=True, return_df=True),\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'datetime': {\n",
    "        'as_number': {\n",
    "            'imputers': [de.DateFeatureExtractor()],\n",
    "            'encoders': [\n",
    "                ne.CyclicalFeatures(drop_original=True)\n",
    "            ]\n",
    "        },\n",
    "        'as_category': {\n",
    "            'imputers': [de.DateFeatureExtractor(dtype='category')],\n",
    "            'encoders': [\n",
    "                ne.MultiTargetEncoder(drop_invariant=True, return_df=True),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "429854cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating encoding pipeline\n"
     ]
    }
   ],
   "source": [
    "# Create the encoding pipeline\n",
    "pipeline = create_encoding_pipeline(encoders_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba191270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the splitting scheme to create the sets\n",
    "split_config = {'test_size': 0.2, 'val_size': 0.2, 'shuffle': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fc269fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_config_get = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac07eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration of the dataset\n",
    "dataset_config = {\n",
    "    'from_date': '22-01-2019',\n",
    "    'to_date': '30-12-2023',\n",
    "    'shift': range(1, 14, 1),\n",
    "    'rolling_window': [7, 14],\n",
    "    'freq': '1D',\n",
    "    'split_config': split_config,\n",
    "    'create_X_y': True,\n",
    "    'encoding_pipeline': pipeline,\n",
    "    'targets_names': target_colomns,\n",
    "    'targets_shift': -3,\n",
    "    'targets_rolling_window': 3,\n",
    "    'targets_history_shifts': range(7, 14, 1),\n",
    "    'targets_history_rolling_windows': [7, 14],\n",
    "    'drop_constant_thr':0.65,\n",
    "    'data_dir': root_dir / 'data'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba1312a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root 2024-10-21 12:55:17,879: INFO: Initialisation de la classe BaseTabularDataset\n",
      "root 2024-10-21 12:55:17,882: INFO: Initialisation de la classe hopitalfeatures\n",
      "root 2024-10-21 12:55:17,882: INFO: Initialisation de la classe airqualityfeatures\n",
      "root 2024-10-21 12:55:17,882: INFO: Initialisation de la classe epidemiologicalfeatures\n",
      "root 2024-10-21 12:55:17,883: INFO: Initialisation de la classe googletrendfeatures\n",
      "root 2024-10-21 12:55:17,883: INFO: Initialisation de la classe meteorologicalfeatures\n",
      "root 2024-10-21 12:55:17,883: INFO: Initialisation de la classe sociologicalfeatures\n",
      "root 2024-10-21 12:55:17,883: INFO: Initialisation de la classe sportscompetitionfeatures\n",
      "root 2024-10-21 12:55:17,884: INFO: Initialisation de la classe trafficfeatures\n",
      "root 2024-10-21 12:55:17,884: INFO: Fetching data for hopitalfeatures\n",
      "root 2024-10-21 12:55:17,890: INFO: Fetching data for airqualityfeatures\n",
      "root 2024-10-21 12:55:17,896: INFO: Fetching data for epidemiologicalfeatures\n",
      "root 2024-10-21 12:55:17,902: INFO: Fetching data for googletrendfeatures\n",
      "root 2024-10-21 12:55:17,906: INFO: Fetching data for meteorologicalfeatures\n",
      "root 2024-10-21 12:55:17,911: INFO: Fetching data for sociologicalfeatures\n",
      "root 2024-10-21 12:55:17,916: INFO: Fetching data for sportscompetitionfeatures\n",
      "root 2024-10-21 12:55:17,919: INFO: Fetching data for trafficfeatures\n",
      "root 2024-10-21 12:55:17,922: INFO: Getting the dataset from 22-01-2019 to 30-12-2023...\n",
      "root 2024-10-21 12:55:17,924: INFO: Getting data for hopitalfeatures from 2019-01-22 00:00:00 to 2023-12-30 00:00:00, at a 1D frequency\n",
      "root 2024-10-21 12:55:17,928: INFO: Augmentation des features...\n",
      "root 2024-10-21 12:55:17,934: INFO: Getting data for airqualityfeatures from 2019-01-22 00:00:00 to 2023-12-30 00:00:00, at a 1D frequency\n",
      "root 2024-10-21 12:55:17,942: INFO: Augmentation des features...\n",
      "root 2024-10-21 12:55:17,967: INFO: Getting data for epidemiologicalfeatures from 2019-01-22 00:00:00 to 2023-12-30 00:00:00, at a 1D frequency\n",
      "root 2024-10-21 12:55:17,972: INFO: Augmentation des features...\n",
      "root 2024-10-21 12:55:17,988: INFO: Getting data for googletrendfeatures from 2019-01-22 00:00:00 to 2023-12-30 00:00:00, at a 1D frequency\n",
      "root 2024-10-21 12:55:17,990: INFO: Augmentation des features...\n",
      "root 2024-10-21 12:55:17,991: INFO: Getting data for meteorologicalfeatures from 2019-01-22 00:00:00 to 2023-12-30 00:00:00, at a 1D frequency\n",
      "root 2024-10-21 12:55:17,996: INFO: Augmentation des features...\n",
      "root 2024-10-21 12:55:18,021: INFO: Getting data for sociologicalfeatures from 2019-01-22 00:00:00 to 2023-12-30 00:00:00, at a 1D frequency\n",
      "root 2024-10-21 12:55:18,025: INFO: Augmentation des features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maxime/Documents/WORKSPACES/forecasting_models/data\n",
      "Dropped columns with zero variance: []\n",
      "Column 'PM10_FR26010' is constant at 15.0 for 68.79% of the rows.\n",
      "Column 'PM10_FR26094' is constant at 19.0 for 88.06% of the rows.\n",
      "Column 'PM25_FR26094' is constant at 6.8 for 87.58% of the rows.\n",
      "Column 'NO2_FR26010' is constant at 6.9 for 68.62% of the rows.\n",
      "Column 'NO2_FR26094' is constant at 7.0 for 87.61% of the rows.\n",
      "Dropped columns with zero variance: ['PM10_FR26010', 'PM10_FR26094', 'PM25_FR26094', 'NO2_FR26010', 'NO2_FR26094']\n",
      "Dropped columns with zero variance: []\n",
      "Dropped columns with zero variance: []\n",
      "Column 'meteo_CHU DIJON_0_snow' is constant at 0.0 for 97.61% of the rows.\n",
      "Dropped columns with zero variance: ['meteo_CHU DIJON_0_snow']\n",
      "Dropped columns with zero variance: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root 2024-10-21 12:55:18,061: INFO: Getting data for sportscompetitionfeatures from 2019-01-22 00:00:00 to 2023-12-30 00:00:00, at a 1D frequency\n",
      "root 2024-10-21 12:55:18,064: INFO: Augmentation des features...\n",
      "root 2024-10-21 12:55:18,069: INFO: Getting data for trafficfeatures from 2019-01-22 00:00:00 to 2023-12-30 00:00:00, at a 1D frequency\n",
      "root 2024-10-21 12:55:18,071: INFO: Augmentation des features...\n",
      "root 2024-10-21 12:55:18,085: WARNING: Not shifting the target is not allowed as some features might not be available,\n",
      "Will use a default shift of -1, matching the value for the next sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns with zero variance: []\n",
      "Column 'nb_accidents' is constant at 0.0 for 76.37% of the rows.\n",
      "Dropped columns with zero variance: ['nb_accidents']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root 2024-10-21 12:55:18,085: INFO: Creating the target columns as nb_emmergencies_CHU Dijon shifted by -1\n",
      "root 2024-10-21 12:55:18,086: INFO: Creating target history columns...\n",
      "root 2024-10-21 12:55:18,097: INFO: Calculating train/val/test sets and encodings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped constant columns from both sets: []\n",
      "X shape: (1153, 526), y shape: (1153, 1)\n",
      "[FeatureUnion]  (step 1 of 4) Processing columntransformer-1, total=   0.0s\n",
      "[FeatureUnion]  (step 2 of 4) Processing columntransformer-2, total=   0.1s\n",
      "[FeatureUnion]  (step 3 of 4) Processing columntransformer-3, total=   0.0s\n",
      "[FeatureUnion]  (step 4 of 4) Processing columntransformer-4, total=   0.0s\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset and fetch the data from the source then call get_dataset() method to fill the different attributes (X and y) of the different sets, and their encodings\n",
    "arsTabularDataset = BaseTabularDataset(features_class=ars_features_class, logger=logger, fetch_config=fetch_config, getter_config=dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee42c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(arsTabularDataset.data.columns.to_list())\n",
    "# Define the model parameters\n",
    "model_params = {\n",
    "    'early_stopping_rounds': 10,\n",
    "    # 'eval_set': [(arsTabularDataset.enc_X_val, arsTabularDataset.y_val)], # TODO: to be set in the experiment's run method\n",
    "    'verbosity': 0,\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': 0.1,\n",
    "    'min_child_weight': 5,\n",
    "    # 'multi_strategy': 'one_output_per_tree',\n",
    "    # 'multi_strategy': 'multi_output_tree' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbf1d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['w_rmse', 'pw_rmse', 'rmse', 'mae', 'mse'] # first one is used for evaluation and everywhere a sinlge metric is used, the rest are used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52e1d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = get_model(model_type='xgboost', name='XGBoost', device='cuda', task_type='regression', test_metrics=metrics, params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e243fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "ars_experiment = BaseExperiment(logger=logger, dataset=arsTabularDataset, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66c8a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model fitting config\n",
    "grid_params = {\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86be32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'verbose': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7c25cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config={\"optimization\": \"grid\", \"grid_params\": grid_params, \"fit_params\": fit_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "ars_experiment.run(dataset_config=dataset_config, model_config=model_config, find_best_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = arsTabularDataset.X_train['Total_CHU Dijon']\n",
    "# val = arsTabularDataset.X_val['Total_CHU Dijon']\n",
    "# df = pd.concat([train, val])\n",
    "# df = df.reset_index()\n",
    "# df.rename({\"Total_CHU Dijon\": \"y\", 'date': 'ds'}, axis=1, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca79ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = arsTabularDataset.X_test['Total_CHU Dijon']\n",
    "future = test.reset_index()\n",
    "future.rename({\"Total_CHU Dijon\": \"y\", 'date':  'ds'}, axis=1, inplace=True)\n",
    "future.drop(columns=[\"y\"], inplace=True)\n",
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761770d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "m = Prophet()\n",
    "m.add_country_holidays(country_name='FR')\n",
    "m.fit(df)\n",
    "m.train_holiday_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = m.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_frame().plot(ax=ax, color='orange')\n",
    "m.plot(forecast, ax=ax, uncertainty=False, xlabel='Date', ylabel='Value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8223c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07589e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26fa78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "fig = plot_plotly(m, forecast)\n",
    "fig.write_html(\"prophet.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet.make_holidays import get_holiday_names, make_holidays_df\n",
    "\n",
    "fr_holidays = make_holidays_df(\n",
    "    year_list=[2019 + i for i in range(10)], country='FR'\n",
    ")\n",
    "\n",
    "fr_holidays_names = get_holiday_names('FR')\n",
    "fr_holidays_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f707d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = arsTabularDataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaadb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac08b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pmdarima as pm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Génération d'une série temporelle synthétique\n",
    "data = arsTabularDataset.data['Total_CHU Dijon']\n",
    "data_exog = arsTabularDataset.enc_data[[col for col in arsTabularDataset.enc_data.columns.to_list() if not col.startswith('target') and not col.startswith('Total_CHU Dijon')]]\n",
    "dates = arsTabularDataset.data.index\n",
    "series = pd.Series(data, index=dates)\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(len(series) * 0.8)\n",
    "train, test = series[:train_size], series[train_size:]\n",
    "train_exog, test_exog = data_exog[:train_size], data_exog[train_size:]\n",
    "\n",
    "# Fonction d'évaluation des modèles\n",
    "def evaluate_forecast(true, predicted):\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    return mse\n",
    "\n",
    "# 1. Modèle ARIMA\n",
    "def fit_arima(train, test):\n",
    "    model = ARIMA(train, order=(5, 1, 0))  # Paramètres (p,d,q)\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=len(test))\n",
    "    evaluate_forecast(test, forecast)\n",
    "    return model_fit, forecast\n",
    "\n",
    "# 2. Modèle SARIMA (avec saisonnalité)\n",
    "def fit_sarima(train, test):\n",
    "    model = pm.auto_arima(train, seasonal=True, m=12, stepwise=True, suppress_warnings=True)\n",
    "    forecast = model.predict(n_periods=len(test))\n",
    "    evaluate_forecast(test, forecast)\n",
    "    return model, forecast\n",
    "\n",
    "# 3. Modèle SARIMAX (avec exogènes)\n",
    "def fit_sarimax(train, test):\n",
    "    # exog = np.random.randn(len(train))  # Exemple de variable exogène\n",
    "    # exog_test = np.random.randn(len(test))\n",
    "    model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12), exog=train_exog)\n",
    "    model_fit = model.fit(disp=False)\n",
    "    forecast = model_fit.forecast(steps=len(test), exog=test_exog)\n",
    "    evaluate_forecast(test, forecast)\n",
    "    return model_fit, forecast\n",
    "\n",
    "# 4. Modèle auto ARIMA (avec auto-ajustement des paramètres)\n",
    "def fit_auto_arima(train, test):\n",
    "    model = pm.auto_arima(train, start_p=1, start_q=1, max_p=5, max_q=5, \n",
    "                          seasonal=False, stepwise=True, suppress_warnings=True)\n",
    "    forecast = model.predict(n_periods=len(test))\n",
    "    evaluate_forecast(test, forecast)\n",
    "    return model, forecast\n",
    "\n",
    "# 5. Modèle Holt-Winters (pour la décomposition additive ou multiplicative)\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "def fit_holt_winters(train, test):\n",
    "    model = ExponentialSmoothing(train, seasonal='add', seasonal_periods=12)\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=len(test))\n",
    "    evaluate_forecast(test, forecast)\n",
    "    return model_fit, forecast\n",
    "\n",
    "# Appel des fonctions\n",
    "print(\"ARIMA:\")\n",
    "fit_arima(train, test)\n",
    "\n",
    "print(\"\\nSARIMA:\")\n",
    "fit_sarima(train, test)\n",
    "\n",
    "print(\"\\nSARIMAX:\")\n",
    "fit_sarimax(train, test)\n",
    "\n",
    "print(\"\\nAuto ARIMA:\")\n",
    "fit_auto_arima(train, test)\n",
    "\n",
    "print(\"\\nHolt-Winters:\")\n",
    "fit_holt_winters(train, test)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
